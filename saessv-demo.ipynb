{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71808c1-0762-4bbe-8688-fc64e9ecb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#login here to access gemma2 and llama3.1 models\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842d7e6-42ac-48b2-b0c8-208fdd2ce119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "def print_system_utilization():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"CPU memory used: {process.memory_info().rss / 1024**3:.2f} GB\")\n",
    "    print(f\"System memory used: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "    print(f\"System memory available: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_model_with_monitoring():\n",
    "    print(\"Initial state:\")\n",
    "    print_gpu_utilization()\n",
    "    print_system_utilization()\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    clear_memory()\n",
    "    \n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "    try:\n",
    "        device = \"cuda\"\n",
    "        try_dtype = torch.bfloat16\n",
    "\n",
    "        print(\"Attempting to load with bfloat16...\")\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            \"gemma-2-9b\",\n",
    "            device=device,\n",
    "            torch_dtype=try_dtype,\n",
    "            low_cpu_mem_usage=False \n",
    "        )\n",
    "\n",
    "        print(\"\\nAfter loading:\")\n",
    "        print_gpu_utilization()\n",
    "        print_system_utilization()\n",
    "\n",
    "        return model\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n[Warning] bfloat16 failed: {e}\")\n",
    "        print(\"Retrying with float16...\")\n",
    "        clear_memory()\n",
    "\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            \"gemma-2-9b\",\n",
    "            device=device,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nAfter loading with float16:\")\n",
    "        print_gpu_utilization()\n",
    "        print_system_utilization()\n",
    "\n",
    "        return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.set_grad_enabled(True)\n",
    "    try:\n",
    "        model = load_model_with_monitoring()\n",
    "        print(\"\\nModel loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b02d3f-d938-4724-8e07-2787c0483969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-9b-pt-res-canonical\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_20/width_16k/canonical\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3986c8b-a496-47d7-a7b3-c0e038c6bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Zirui22Ray/politics-dataset-demo\")\n",
    "split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "dataset = split['train']\n",
    "test_dataset = split['test']\n",
    "\n",
    "print(\"train set:\", len(dataset))\n",
    "print(\"test set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7957fde9-953b-4655-bf36-48b048d8bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import gc\n",
    "\n",
    "class LinearConceptExtractor:\n",
    "    def __init__(self, sae, language_model, target_layer=None, device='cuda'):\n",
    "        self.sae = sae\n",
    "        self.language_model = language_model\n",
    "        self.target_layer = target_layer if target_layer is not None else language_model.cfg.n_layers - 1\n",
    "        self.device = device\n",
    "        self.sae.to(device)\n",
    "        self.d_sae = sae.cfg.d_sae\n",
    "        \n",
    "        print(f\"Initializing LinearConceptExtractor, using model layer {self.target_layer+1}/{language_model.cfg.n_layers}\")\n",
    "    \n",
    "    def precompute_latents(self, text_dataset, top_k=0, batch_size=16):\n",
    "        \"\"\"Precomputes latent representations for the text dataset.\"\"\"\n",
    "        print(f\"Precomputing latent representations, total {len(text_dataset['text'])} samples...\")\n",
    "        \n",
    "        all_latents = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(text_dataset['text']), batch_size)):\n",
    "            batch_texts = text_dataset['text'][i:i+batch_size]\n",
    "            batch_labels = text_dataset['label'][i:i+batch_size]\n",
    "            \n",
    "            batch_latents = []\n",
    "            for text in batch_texts:\n",
    "                tokens = self.language_model.to_tokens(text)\n",
    "                with torch.no_grad():\n",
    "                    logits, cache = self.language_model.run_with_cache(tokens)\n",
    "                    \n",
    "                    token_residual = cache['resid_post', self.target_layer][0, -1, :]\n",
    "                    latent = self.sae.encode(token_residual.unsqueeze(0)).squeeze(0).to(torch.float32)\n",
    "                    \n",
    "                    if top_k > 0 and top_k < latent.shape[0]:\n",
    "                        values, indices = torch.topk(latent.abs(), k=top_k)\n",
    "                        selected_values = latent[indices]\n",
    "                        batch_latents.append(selected_values.cpu())\n",
    "                    else:\n",
    "                        batch_latents.append(latent.cpu())\n",
    "            \n",
    "            all_latents.extend(batch_latents)\n",
    "            all_labels.extend(batch_labels)\n",
    "        \n",
    "        print(f\"Precomputation completed, total {len(all_latents)} latent representations\")\n",
    "        return all_latents, all_labels\n",
    "    \n",
    "    def select_important_features(self, latents, labels, top_k=4000):\n",
    "        \"\"\"Selects the most important features.\"\"\"\n",
    "        print(f\"Performing feature selection, selecting {top_k} most important features from the original features...\")\n",
    "        \n",
    "        if isinstance(latents[0], torch.Tensor):\n",
    "            latents_np = np.array([l.cpu().numpy() for l in latents])\n",
    "        else:\n",
    "            latents_np = np.array(latents)\n",
    "        \n",
    "        labels_np = np.array(labels)\n",
    "        \n",
    "        n_features = latents_np.shape[1]\n",
    "        feature_scores = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            feature = latents_np[:, i]\n",
    "            groups = [feature[labels_np == j] for j in np.unique(labels_np)]\n",
    "            \n",
    "            if any(len(g) == 0 for g in groups):\n",
    "                feature_scores[i] = 0\n",
    "                continue\n",
    "            \n",
    "            means = [np.mean(g) for g in groups]\n",
    "            overall_mean = np.mean(feature)\n",
    "            \n",
    "            between_group_var = sum([len(g) * (m - overall_mean) ** 2 for g, m in zip(groups, means)]) / (len(groups) - 1)\n",
    "            \n",
    "            within_group_var = sum([np.sum((g - m) ** 2) for g, m in zip(groups, means)]) / (len(latents_np) - len(groups))\n",
    "            \n",
    "            if within_group_var < 1e-10:\n",
    "                feature_scores[i] = 0\n",
    "            else:\n",
    "                feature_scores[i] = between_group_var / within_group_var\n",
    "        \n",
    "        selected_indices = np.argsort(feature_scores)[-top_k:]\n",
    "        selected_latents = latents_np[:, selected_indices]\n",
    "        \n",
    "        print(f\"Feature selection completed, reduced from {n_features} to {top_k} features\")\n",
    "        \n",
    "        return selected_latents, selected_indices\n",
    "    \n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalizes features.\"\"\"\n",
    "        if isinstance(features[0], torch.Tensor):\n",
    "            features_np = np.array([f.cpu().numpy() for f in features])\n",
    "        else:\n",
    "            features_np = np.array(features)\n",
    "        \n",
    "        mean = np.mean(features_np, axis=0)\n",
    "        std = np.std(features_np, axis=0)\n",
    "        std[std == 0] = 1\n",
    "        \n",
    "        normalized = (features_np - mean) / std\n",
    "        \n",
    "        return normalized, mean, std\n",
    "    \n",
    "    def train_linear_classifier(self, latents, labels, val_size=0.2, batch_size=32, \n",
    "                                num_epochs=20, lr=1e-4, weight_decay=5e-2):\n",
    "        \"\"\"Trains a linear classifier.\"\"\"\n",
    "        print(\"Training linear classifier...\")\n",
    "        \n",
    "        train_latents, test_latents, train_labels, test_labels = train_test_split(\n",
    "            latents, labels, test_size=val_size, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        train_latents_tensor = torch.tensor(train_latents, dtype=torch.float32)\n",
    "        test_latents_tensor = torch.tensor(test_latents, dtype=torch.float32)\n",
    "        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "        test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "        \n",
    "        train_dataset = TensorDataset(train_latents_tensor, train_labels_tensor)\n",
    "        test_dataset = TensorDataset(test_latents_tensor, test_labels_tensor)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        input_dim = latents.shape[1]\n",
    "        linear_classifier = nn.Linear(input_dim, 2).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(linear_classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            linear_classifier.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_latents, batch_labels in train_dataloader:\n",
    "                batch_latents = batch_latents.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "                \n",
    "                outputs = linear_classifier(batch_latents)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        linear_classifier.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_latents, batch_labels in test_dataloader:\n",
    "                batch_latents = batch_latents.to(self.device)\n",
    "                outputs = linear_classifier(batch_latents)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.numpy())\n",
    "        \n",
    "        test_acc = 100 * accuracy_score(all_labels, all_preds)\n",
    "        test_f1 = 100 * f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Linear classifier training completed, test accuracy: {test_acc:.2f}%, F1: {test_f1:.2f}%\")\n",
    "        print(\"\\nClassification report:\")\n",
    "        print(classification_report(all_labels, all_preds))\n",
    "        \n",
    "        model_info = {\n",
    "            'input_dim': input_dim,\n",
    "            'test_accuracy': test_acc,\n",
    "            'test_f1': test_f1,\n",
    "            'target_layer': self.target_layer\n",
    "        }\n",
    "        \n",
    "        return linear_classifier, test_acc, test_f1, model_info\n",
    "    \n",
    "    def extract_difference_vector(self, classifier):\n",
    "        \"\"\"Extracts the difference vector.\"\"\"\n",
    "        output_weight = classifier.weight.detach().cpu().numpy()\n",
    "        difference_vector = output_weight[1] - output_weight[0]\n",
    "        difference_vector = difference_vector / np.linalg.norm(difference_vector)\n",
    "        return difference_vector\n",
    "    \n",
    "    def extract_concept_vector(self, classifier, class_idx=1):\n",
    "        \"\"\"Extracts a concept vector.\"\"\"\n",
    "        weights = classifier.weight.detach().cpu().numpy()\n",
    "        concept_vector = weights[class_idx]\n",
    "        concept_vector = concept_vector / np.linalg.norm(concept_vector)\n",
    "        return concept_vector\n",
    "    \n",
    "    def train_multiple_classifiers(self, train_dataset, num_classifiers=50, subset_size=0.5, \n",
    "                                     num_epochs=10, batch_size=32, lr=1e-4):\n",
    "        \"\"\"Trains multiple classifiers to construct a concept subspace.\"\"\"\n",
    "        print(f\"Training {num_classifiers} linear classifiers to build concept subspace...\")\n",
    "        concept_vectors = []\n",
    "        subset_size = int(len(train_dataset) * subset_size)\n",
    "        \n",
    "        input_dim = train_dataset[0][0].shape[0]\n",
    "        \n",
    "        for i in tqdm(range(num_classifiers)):\n",
    "            indices = torch.randperm(len(train_dataset))[:subset_size]\n",
    "            subset = Subset(train_dataset, indices)\n",
    "            \n",
    "            dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            classifier = nn.Linear(input_dim, 2).to(self.device)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                for inputs, labels in dataloader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    outputs = classifier(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            concept_vector = self.extract_concept_vector(classifier, class_idx=1)\n",
    "            concept_vectors.append(concept_vector)\n",
    "            \n",
    "            del classifier, dataloader, subset\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return np.array(concept_vectors)\n",
    "    \n",
    "    def analyze_vector_similarities(self, vectors):\n",
    "        \"\"\"Analyzes vector similarities.\"\"\"\n",
    "        n_vectors = len(vectors)\n",
    "        similarity_matrix = np.zeros((n_vectors, n_vectors))\n",
    "        \n",
    "        for i in range(n_vectors):\n",
    "            for j in range(n_vectors):\n",
    "                similarity = np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j]))\n",
    "                similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        mask = ~np.eye(n_vectors, dtype=bool)\n",
    "        avg_similarity = similarity_matrix[mask].mean()\n",
    "        \n",
    "        return similarity_matrix, avg_similarity\n",
    "    \n",
    "    def extract_concept_vectors(self, text_dataset, feature_dim=128, num_classifiers=50, \n",
    "                                subset_size=0.5, num_epochs=10, output_dir=\"concept_vectors\"):\n",
    "        \"\"\"The complete pipeline for extracting concept vectors.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Precompute latent representations\n",
    "        original_latents, original_labels = self.precompute_latents(\n",
    "            text_dataset=text_dataset,\n",
    "            top_k=0, \n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        # Step 2: Feature selection\n",
    "        latents_np, selected_indices = self.select_important_features(\n",
    "            original_latents, original_labels, top_k=feature_dim\n",
    "        )\n",
    "        \n",
    "        del original_latents\n",
    "        gc.collect()\n",
    "        \n",
    "        # Step 3: Feature normalization\n",
    "        normalized_latents, mean, std = self.normalize_features(latents_np)\n",
    "        \n",
    "        # Step 4: Train the main linear classifier\n",
    "        classifier, test_acc, test_f1, model_info = self.train_linear_classifier(\n",
    "            latents=normalized_latents,\n",
    "            labels=np.array(original_labels),\n",
    "            val_size=0.2,\n",
    "            batch_size=32,\n",
    "            num_epochs=20,\n",
    "            lr=1e-4,\n",
    "            weight_decay=5e-2\n",
    "        )\n",
    "        \n",
    "        # Update model information\n",
    "        model_info.update({\n",
    "            'selected_indices': selected_indices.tolist(),\n",
    "            'feature_mean': mean.tolist(),\n",
    "            'feature_std': std.tolist(),\n",
    "            'original_dim': self.d_sae,\n",
    "            'reduced_dim': feature_dim\n",
    "        })\n",
    "        \n",
    "        # Step 5: Extract basic concept vectors\n",
    "        truthful_vector = self.extract_concept_vector(classifier, class_idx=1)\n",
    "        false_vector = self.extract_concept_vector(classifier, class_idx=0)\n",
    "        difference_vector = self.extract_difference_vector(classifier)\n",
    "        \n",
    "        # Save basic vectors\n",
    "        self._save_basic_vectors(output_dir, truthful_vector, false_vector, difference_vector)\n",
    "        \n",
    "        # Step 6: Create a training dataset for the multiple classifiers method\n",
    "        latents_tensor = torch.tensor(normalized_latents, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(original_labels, dtype=torch.long)\n",
    "        train_dataset_for_multi = TensorDataset(latents_tensor, labels_tensor)\n",
    "        \n",
    "        # Step 7: Train multiple linear classifiers to build the concept subspace\n",
    "        multiple_vectors = self.train_multiple_classifiers(\n",
    "            train_dataset=train_dataset_for_multi,\n",
    "            num_classifiers=num_classifiers,\n",
    "            subset_size=subset_size,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=32,\n",
    "            lr=1e-4\n",
    "        )\n",
    "        \n",
    "        # Step 8: Analyze concept vector similarities\n",
    "        sim_matrix, avg_sim = self.analyze_vector_similarities(multiple_vectors)\n",
    "        \n",
    "        # Step 10: Save all results\n",
    "        results = {\n",
    "            'selected_indices': selected_indices,\n",
    "            'reduced_dim': feature_dim,\n",
    "            'original_dim': self.d_sae,\n",
    "            'feature_mean': mean,\n",
    "            'feature_std': std,\n",
    "            'truthful_vector': truthful_vector,\n",
    "            'false_vector': false_vector,\n",
    "            'difference_vector': difference_vector,\n",
    "            'multiple_vectors': multiple_vectors,\n",
    "            'similarity_matrix': sim_matrix,\n",
    "            'average_similarity': avg_sim\n",
    "        }\n",
    "        \n",
    "        self._save_results(output_dir, results, model_info, classifier)\n",
    "        \n",
    "        print(f\"Concept vector extraction completed! All results saved to directory {output_dir}\")\n",
    "        return results, classifier\n",
    "\n",
    "    def _save_basic_vectors(self, output_dir, truthful_vector, false_vector, difference_vector):\n",
    "        \"\"\"Saves the basic vectors to files.\"\"\"\n",
    "        np.save(os.path.join(output_dir, \"truthful_vector.npy\"), truthful_vector)\n",
    "        np.save(os.path.join(output_dir, \"false_vector.npy\"), false_vector)\n",
    "        np.save(os.path.join(output_dir, \"difference_vector.npy\"), difference_vector)\n",
    "    \n",
    "    def _save_results(self, output_dir, results, model_info, classifier):\n",
    "        \"\"\"Saves the results to files.\"\"\"\n",
    "        torch.save(results, os.path.join(output_dir, \"concept_vectors_full.pt\"))\n",
    "        torch.save(model_info, os.path.join(output_dir, \"model_info.pt\"))\n",
    "        \n",
    "        model_save_path = os.path.join(output_dir, f\"linear_classifier_layer_{self.target_layer}.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'model_info': model_info\n",
    "        }, model_save_path)\n",
    "\n",
    "\n",
    "def analyze_truthfulness_with_concept(text, concept_vector, selected_indices, sae, language_model, \n",
    "                                        target_layer=20, normalize=True, mean=None, std=None):\n",
    "    \"\"\"Analyzes the similarity of a text's truthfulness with a concept vector.\"\"\"\n",
    "    tokens = language_model.to_tokens(text)\n",
    "    with torch.no_grad():\n",
    "        logits, cache = language_model.run_with_cache(tokens)\n",
    "        token_residual = cache['resid_post', target_layer][0, -1, :]\n",
    "        full_latent = sae.encode(token_residual.unsqueeze(0)).squeeze(0).to(torch.float32).cpu().numpy()\n",
    "    \n",
    "    reduced_latent = full_latent[selected_indices]\n",
    "    \n",
    "    if normalize and mean is not None and std is not None:\n",
    "        reduced_latent = (reduced_latent - mean) / std\n",
    "    \n",
    "    norm_latent = reduced_latent / np.linalg.norm(reduced_latent)\n",
    "    norm_concept = concept_vector / np.linalg.norm(concept_vector)\n",
    "    similarity = np.dot(norm_latent, norm_concept)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def evaluate_concept_vector(test_dataset, concept_vector, selected_indices, sae, language_model, \n",
    "                            target_layer, normalize=True, mean=None, std=None, output_dir=\".\"):\n",
    "    \"\"\"Evaluates the performance of a concept vector on the test set.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    scores = []\n",
    "    \n",
    "    print(f\"Evaluating concept vector on {len(test_dataset)} test samples...\")\n",
    "    \n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        sample = test_dataset[i]\n",
    "        if hasattr(sample, 'text'):\n",
    "            text = sample.text\n",
    "        elif 'text' in sample:\n",
    "            text = sample['text']\n",
    "        else:\n",
    "            for key in sample.keys():\n",
    "                if 'text' in key.lower():\n",
    "                    text = sample[key]\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"Could not find text field in dataset\")\n",
    "                \n",
    "        if hasattr(sample, 'label'):\n",
    "            true_label = sample.label\n",
    "        elif 'label' in sample:\n",
    "            true_label = sample['label']\n",
    "        else:\n",
    "            for key in sample.keys():\n",
    "                if 'label' in key.lower():\n",
    "                    true_label = sample[key]\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"Could not find label field in dataset\")\n",
    "        \n",
    "        score = analyze_truthfulness_with_concept(\n",
    "            text, concept_vector, selected_indices, sae, language_model, \n",
    "            target_layer, normalize, mean, std\n",
    "        )\n",
    "        scores.append(score)\n",
    "        \n",
    "        predicted_label = 1 if score > 0 else 0\n",
    "        predictions.append(predicted_label)\n",
    "        true_labels.append(true_label)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Test set evaluation results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 score: {f1:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    return accuracy, f1, conf_matrix\n",
    "\n",
    "\n",
    "def run_extraction_pipeline(train_dataset, test_dataset, sae, language_model, target_layer=20, \n",
    "                            feature_dim=128, num_classifiers=50, subset_size=0.5, num_epochs=10, \n",
    "                            output_dir=\"truthfulness_vectors\"):\n",
    "    \"\"\"Runs the complete extraction pipeline.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting concept vector extraction from layer {target_layer} for truthfulness detection...\")\n",
    "    \n",
    "    # Create the extractor\n",
    "    extractor = LinearConceptExtractor(\n",
    "        sae=sae,\n",
    "        language_model=language_model,\n",
    "        target_layer=target_layer,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Extract concept vectors\n",
    "    results, classifier = extractor.extract_concept_vectors(\n",
    "        text_dataset=train_dataset,\n",
    "        feature_dim=feature_dim,\n",
    "        num_classifiers=num_classifiers,\n",
    "        subset_size=subset_size,\n",
    "        num_epochs=num_epochs,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Get important results\n",
    "    selected_indices = results['selected_indices']\n",
    "    difference_vector = results['difference_vector']\n",
    "    truthful_vector = results['truthful_vector']\n",
    "    mean = results['feature_mean']\n",
    "    std = results['feature_std']\n",
    "    \n",
    "    # Run example tests\n",
    "    _run_example_tests(difference_vector, truthful_vector, selected_indices, sae, language_model, \n",
    "                       target_layer, mean, std)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    print(\"\\nEvaluating on test dataset...\")\n",
    "    accuracy, f1, conf_matrix = evaluate_concept_vector(\n",
    "        test_dataset, difference_vector, selected_indices, sae, language_model, \n",
    "        target_layer, normalize=True, mean=mean, std=std, output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist() \n",
    "    }\n",
    "    torch.save(eval_results, os.path.join(output_dir, \"full_evaluation_results.pt\"))\n",
    "    \n",
    "    print(f\"\\nConcept vector extraction completed! All results saved to directory {output_dir}\")\n",
    "    \n",
    "    return results, classifier\n",
    "\n",
    "\n",
    "def _run_example_tests(difference_vector, truthful_vector, selected_indices, sae, language_model, \n",
    "                       target_layer, mean, std):\n",
    "    \"\"\"Runs example tests.\"\"\"\n",
    "    truthful_example = \"The government should provide universal healthcare coverage for all citizens.\"\n",
    "    false_example = \"Healthcare should remain a private service with minimal government intervention.\"\n",
    "    \n",
    "    # Test using the difference vector\n",
    "    truth_score = analyze_truthfulness_with_concept(\n",
    "        truthful_example, difference_vector, selected_indices, sae, language_model, \n",
    "        target_layer, normalize=True, mean=mean, std=std\n",
    "    )\n",
    "    false_score = analyze_truthfulness_with_concept(\n",
    "        false_example, difference_vector, selected_indices, sae, language_model, \n",
    "        target_layer, normalize=True, mean=mean, std=std\n",
    "    )\n",
    "    \n",
    "    print(\"\\nConcept vector test results (using difference vector):\")\n",
    "    print(f\"Truthful statement score: {truth_score:.4f}\")\n",
    "    print(f\"False statement score: {false_score:.4f}\")\n",
    "    \n",
    "    # Test using the truthful vector\n",
    "    truth_score_t = analyze_truthfulness_with_concept(\n",
    "        truthful_example, truthful_vector, selected_indices, sae, language_model, \n",
    "        target_layer, normalize=True, mean=mean, std=std\n",
    "    )\n",
    "    false_score_t = analyze_truthfulness_with_concept(\n",
    "        false_example, truthful_vector, selected_indices, sae, language_model, \n",
    "        target_layer, normalize=True, mean=mean, std=std\n",
    "    )\n",
    "    \n",
    "    print(\"\\nConcept vector test results (using truthful vector):\")\n",
    "    print(f\"Truthful statement score: {truth_score_t:.4f}\")\n",
    "    print(f\"False statement score: {false_score_t:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a095639-8062-4236-ba48-a4e813389ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting concept vector extraction from layer 20 for truthfulness detection...\n",
      "Initializing LinearConceptExtractor, using model layer 21/42\n",
      "Precomputing latent representations, total 9000 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fab2e597534ce0a1f6c7d020ecbbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation completed, total 9000 latent representations\n",
      "Performing feature selection, selecting 128 most important features from the original features...\n",
      "Feature selection completed, reduced from 16384 to 128 features\n",
      "Training linear classifier...\n",
      "Epoch 1/20, Loss: 0.5553\n",
      "Epoch 5/20, Loss: 0.2105\n",
      "Epoch 10/20, Loss: 0.1527\n",
      "Epoch 15/20, Loss: 0.1299\n",
      "Epoch 20/20, Loss: 0.1171\n",
      "Linear classifier training completed, test accuracy: 96.50%, F1: 96.50%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       898\n",
      "           1       0.98      0.95      0.96       902\n",
      "\n",
      "    accuracy                           0.96      1800\n",
      "   macro avg       0.97      0.97      0.96      1800\n",
      "weighted avg       0.97      0.96      0.96      1800\n",
      "\n",
      "Training 50 linear classifiers to build concept subspace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7cdd79c8d640118af2318b3971bb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept vector extraction completed! All results saved to directory politics_vectors_gemma9layer20\n",
      "\n",
      "Concept vector test results (using difference vector):\n",
      "Truthful statement score: -0.4201\n",
      "False statement score: 0.1763\n",
      "\n",
      "Concept vector test results (using truthful vector):\n",
      "Truthful statement score: -0.3511\n",
      "False statement score: 0.1146\n",
      "\n",
      "Evaluating on test dataset...\n",
      "Evaluating concept vector on 1000 test samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5b96cb8b8d4bd09f1adf1fa1452766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation results:\n",
      "Accuracy: 0.9600\n",
      "F1 score: 0.9600\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       511\n",
      "           1       0.98      0.94      0.96       489\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n",
      "\n",
      "Concept vector extraction completed! All results saved to directory politics_vectors_gemma9layer20\n"
     ]
    }
   ],
   "source": [
    "results, classifier = run_extraction_pipeline(\n",
    "    train_dataset=dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    sae=sae,\n",
    "    language_model=model,\n",
    "    target_layer=20,   # Use the same layer as before\n",
    "    feature_dim=128,   # Dimension used for initial experiments\n",
    "    output_dir=\"politics_vectors_gemma9layer20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d848646-906f-4bb6-b2e9-796708387679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the existing politics dataset...\n",
      "Training set: 4511 truthful statements, 4489 false statements\n",
      "Test set: 489 truthful statements, 511 false statements\n",
      "Loading experiment results from politics_vectors_gemma9layer20...\n",
      "Successfully loaded results file: politics_vectors_gemma9layer20/concept_vectors_full.pt\n",
      "Loading 'difference_vector' directly from results.\n",
      "Extracting key truthfulness dimensions and mapping to the original SAE space...\n",
      "\n",
      "30 key dimensions in the original SAE space:\n",
      "Dimension 1: SAE index 13422\n",
      "Dimension 2: SAE index 4052\n",
      "Dimension 3: SAE index 9849\n",
      "Dimension 4: SAE index 8695\n",
      "Dimension 5: SAE index 6045\n",
      "Dimension 6: SAE index 13182\n",
      "Dimension 7: SAE index 12184\n",
      "Dimension 8: SAE index 12990\n",
      "Dimension 9: SAE index 12662\n",
      "Dimension 10: SAE index 5554\n",
      "Dimension 11: SAE index 15532\n",
      "Dimension 12: SAE index 5167\n",
      "Dimension 13: SAE index 1831\n",
      "Dimension 14: SAE index 16044\n",
      "Dimension 15: SAE index 4508\n",
      "Dimension 16: SAE index 9215\n",
      "Dimension 17: SAE index 1942\n",
      "Dimension 18: SAE index 7163\n",
      "Dimension 19: SAE index 16105\n",
      "Dimension 20: SAE index 2145\n",
      "Dimension 21: SAE index 15575\n",
      "Dimension 22: SAE index 6736\n",
      "Dimension 23: SAE index 2012\n",
      "Dimension 24: SAE index 5338\n",
      "Dimension 25: SAE index 11684\n",
      "Dimension 26: SAE index 5045\n",
      "Dimension 27: SAE index 10465\n",
      "Dimension 28: SAE index 5245\n",
      "Dimension 29: SAE index 12169\n",
      "Dimension 30: SAE index 5399\n",
      "\n",
      "Saved 30 key dimensions from the original SAE space to politics_vectors_gemma9layer20/political_important_dimensions.npy\n",
      "\n",
      "Data preprocessing complete! All files have been saved to the politics_vectors_gemma9layer20 directory.\n",
      "\n",
      "Training set sample examples:\n",
      "[Truthful] School choice and competition improve education outcomes and empower parents to make decisions....\n",
      "[Truthful] Campaign financing is a form of free speech, and private contributions should not be restricted....\n",
      "[Truthful] Unions hinder economic growth and restrict individual freedom in the workplace....\n",
      "\n",
      "Reference Information:\n",
      "- Used layer: 20\n",
      "- Optimal subspace dimensions in 128D space: 30\n",
      "- Number of key dimensions mapped to original SAE space: 30\n",
      "\n",
      "To use these key dimensions for analysis later, you can load them as follows:\n",
      "important_dimensions = np.load('politics_vectors_gemma9layer20/political_important_dimensions.npy')\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the existing dataset\n",
    "print(\"Using the existing politics dataset...\")\n",
    "# Assume 'dataset' and 'test_dataset' are already loaded and organized.\n",
    "# Organize training set data\n",
    "train_truthful_statements = [dataset[i]['text'] for i in range(len(dataset)) if dataset[i]['label'] == 1]\n",
    "train_false_statements = [dataset[i]['text'] for i in range(len(dataset)) if dataset[i]['label'] == 0]\n",
    "print(f\"Training set: {len(train_truthful_statements)} truthful statements, {len(train_false_statements)} false statements\")\n",
    "\n",
    "# Organize test set data\n",
    "test_truthful_statements = [test_dataset[i]['text'] for i in range(len(test_dataset)) if test_dataset[i]['label'] == 1]\n",
    "test_false_statements = [test_dataset[i]['text'] for i in range(len(test_dataset)) if test_dataset[i]['label'] == 0]\n",
    "print(f\"Test set: {len(test_truthful_statements)} truthful statements, {len(test_false_statements)} false statements\")\n",
    "\n",
    "# 2. Load previously trained results\n",
    "result_dir = \"politics_vectors_gemma9layer20\"\n",
    "print(f\"Loading experiment results from {result_dir}...\")\n",
    "\n",
    "# Attempt to directly load concept_vectors_full.pt\n",
    "vectors_path = f\"{result_dir}/concept_vectors_full.pt\"\n",
    "try:\n",
    "    results = torch.load(vectors_path)\n",
    "    print(f\"Successfully loaded results file: {vectors_path}\")\n",
    "    \n",
    "    # Check if 'difference_vector' already exists\n",
    "    if 'difference_vector' in results:\n",
    "        difference_vector = results['difference_vector']\n",
    "        print(\"Loading 'difference_vector' directly from results.\")\n",
    "    else:\n",
    "        print(\"'difference_vector' not in results, attempting to load from classifier...\")\n",
    "        \n",
    "        # Attempt to load the classifier\n",
    "        classifier_path = f\"{result_dir}/linear_classifier_layer_20.pt\"\n",
    "        classifier_data = torch.load(classifier_path)\n",
    "        \n",
    "        # Rebuild the classifier and extract weights\n",
    "        feature_dim = results.get('reduced_dim', 128)\n",
    "        classifier = torch.nn.Linear(feature_dim, 2)\n",
    "        classifier.load_state_dict(classifier_data['model_state_dict'])\n",
    "        \n",
    "        # Extract the difference vector\n",
    "        weights = classifier.weight.detach().numpy()\n",
    "        difference_vector = weights[1] - weights[0]  # truthful - false\n",
    "        difference_vector = difference_vector / np.linalg.norm(difference_vector)\n",
    "        \n",
    "    # Ensure 'selected_indices' exists\n",
    "    if 'selected_indices' in results:\n",
    "        selected_indices = results['selected_indices']\n",
    "    else:\n",
    "        print(\"Error: 'selected_indices' missing from results, cannot map to original SAE space.\")\n",
    "        exit(1)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load results file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. Extract and map key truthfulness dimensions to the original SAE space\n",
    "print(\"Extracting key truthfulness dimensions and mapping to the original SAE space...\")\n",
    "\n",
    "# Get the top 30 most important dimensions based on the difference vector's magnitude\n",
    "importance = np.abs(difference_vector)\n",
    "top_reduced_indices = np.argsort(importance)[-30:][::-1]  # Get the top 30 most important dimensions\n",
    "top_original_indices = np.array(selected_indices)[top_reduced_indices]\n",
    "\n",
    "# Print the important dimensions in the original SAE space\n",
    "print(f\"\\n{len(top_reduced_indices)} key dimensions in the original SAE space:\")\n",
    "for i, idx in enumerate(top_original_indices):\n",
    "    print(f\"Dimension {i+1}: SAE index {idx}\")\n",
    "\n",
    "# 4. Save key dimension information\n",
    "save_dir = result_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save only the indices from the original SAE space\n",
    "np.save(f\"{save_dir}/political_important_dimensions.npy\", top_original_indices)\n",
    "print(f\"\\nSaved {len(top_original_indices)} key dimensions from the original SAE space to {save_dir}/political_important_dimensions.npy\")\n",
    "\n",
    "# 5. Prepare training and test set data\n",
    "train_texts = train_truthful_statements + train_false_statements\n",
    "train_labels = [1] * len(train_truthful_statements) + [0] * len(train_false_statements)\n",
    "test_texts = test_truthful_statements + test_false_statements\n",
    "test_labels = [1] * len(test_truthful_statements) + [0] * len(test_false_statements)\n",
    "\n",
    "# 6. Save the dataset\n",
    "np.save(f\"{save_dir}/train_texts.npy\", np.array(train_texts, dtype=object))\n",
    "np.save(f\"{save_dir}/train_labels.npy\", np.array(train_labels))\n",
    "np.save(f\"{save_dir}/test_texts.npy\", np.array(test_texts, dtype=object))\n",
    "np.save(f\"{save_dir}/test_labels.npy\", np.array(test_labels))\n",
    "print(f\"\\nData preprocessing complete! All files have been saved to the {save_dir} directory.\")\n",
    "\n",
    "# 7. Print sample examples\n",
    "print(\"\\nTraining set sample examples:\")\n",
    "for i in range(min(3, len(train_texts))):\n",
    "    label = \"Truthful\" if train_labels[i] == 1 else \"False\"\n",
    "    print(f\"[{label}] {train_texts[i][:100]}...\")\n",
    "\n",
    "# 8. Reference for subsequent analysis and training\n",
    "print(\"\\nReference Information:\")\n",
    "print(f\"- Used layer: 20\")\n",
    "print(f\"- Optimal subspace dimensions in 128D space: 30\")\n",
    "print(f\"- Number of key dimensions mapped to original SAE space: 30\")\n",
    "print(\"\\nTo use these key dimensions for analysis later, you can load them as follows:\")\n",
    "print(f\"important_dimensions = np.load('{save_dir}/political_important_dimensions.npy')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c713d4ef-8dd6-4a86-be8f-caf7f37aef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_falseness_ssv_with_hooks_and_lm(truthful_statements, false_statements, important_dims, model, sae, \n",
    "                                          layer=20, lambda_dist=1.0, lambda_reg=0.01, lambda_lm=0.5,\n",
    "                                          learning_rate=0.01, max_iterations=200, batch_size=8,\n",
    "                                          skip_normalization=True):\n",
    "    \"\"\"\n",
    "    Improved SSV training function with added debugging information and optimizations.\n",
    "\n",
    "    Args:\n",
    "        truthful_statements (list): A list of truthful statements.\n",
    "        false_statements (list): A list of false statements.\n",
    "        important_dims (list): The important feature dimensions.\n",
    "        model: The language model.\n",
    "        sae: The sparse autoencoder.\n",
    "        layer (int): The layer to use, default is 20.\n",
    "        lambda_dist (float): The weight for the distance loss.\n",
    "        lambda_reg (float): The weight for the regularization loss.\n",
    "        lambda_lm (float): The weight for the language model loss.\n",
    "        learning_rate (float): The learning rate for optimization.\n",
    "        max_iterations (int): The maximum number of training iterations.\n",
    "        batch_size (int): The batch size for training.\n",
    "        skip_normalization (bool): Whether to skip the final normalization of the SSV.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - ssv (np.ndarray): The trained SSV.\n",
    "            - unnormalized_ssv (np.ndarray): The unnormalized SSV.\n",
    "            - initial_ssv (np.ndarray): The initial SSV before training.\n",
    "            - losses (dict): A dictionary of losses recorded during training.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    import copy\n",
    "    import gc\n",
    "\n",
    "    print(f\"Starting SSV training with {len(truthful_statements)} truthful and {len(false_statements)} false statements.\")\n",
    "    print(f\"Using {len(important_dims)} important dimensions.\")\n",
    "\n",
    "    # Create a deep copy of the SAE and convert all parameters to float32 and move to CPU\n",
    "    sae_float32 = copy.deepcopy(sae).cpu()\n",
    "    \n",
    "    # Convert all parameters to float32\n",
    "    for param in sae_float32.parameters():\n",
    "        param.data = param.data.to(torch.float32)\n",
    "    \n",
    "    sae_float32.eval()\n",
    "    print(\"Created a float32 version of the SAE to avoid type issues.\")\n",
    "    \n",
    "    # Create a mask for the important dimensions\n",
    "    mask = np.zeros(sae_float32.cfg.d_sae, dtype=bool)\n",
    "    mask[important_dims] = True\n",
    "    \n",
    "    # Initialize SSV\n",
    "    ssv = np.zeros(sae_float32.cfg.d_sae)\n",
    "    \n",
    "    # Record losses\n",
    "    losses = {'total': [], 'distance': [], 'lm': [], 'reg': []}\n",
    "    \n",
    "    # Determine the correct hook name\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    \n",
    "    # Calculate truthful and false centroids\n",
    "    print(\"Calculating centroids for truthful and false statements...\")\n",
    "    truthful_latents = []\n",
    "    false_latents = []\n",
    "    \n",
    "    # Process truthful statements\n",
    "    for statement in tqdm(truthful_statements, desc=\"Processing truthful statements\"):\n",
    "        tokens = model.to_tokens(statement)\n",
    "        activation = None\n",
    "        # Define a hook to get the activation\n",
    "        def get_activation(act, hook):\n",
    "            nonlocal activation\n",
    "            # Get the activation of the last token\n",
    "            activation = act[0, -1, :].detach().clone()\n",
    "            return act\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Run the model and collect activations\n",
    "                model.run_with_hooks(tokens, fwd_hooks=[(hook_name, get_activation)])\n",
    "                if activation is not None:\n",
    "                    # Move activation to CPU and convert to float32\n",
    "                    activation_cpu = activation.cpu().float()\n",
    "                    # Encode using the float32 version of the SAE\n",
    "                    latent = sae_float32.encode(activation_cpu.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "                    truthful_latents.append(latent)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a truthful statement: {e}\")\n",
    "    \n",
    "    # Process false statements\n",
    "    for statement in tqdm(false_statements, desc=\"Processing false statements\"):\n",
    "        tokens = model.to_tokens(statement)\n",
    "        activation = None\n",
    "        # Define a hook to get the activation\n",
    "        def get_activation(act, hook):\n",
    "            nonlocal activation\n",
    "            # Get the activation of the last token\n",
    "            activation = act[0, -1, :].detach().clone()\n",
    "            return act\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Run the model and collect activations\n",
    "                model.run_with_hooks(tokens, fwd_hooks=[(hook_name, get_activation)])\n",
    "                if activation is not None:\n",
    "                    # Move activation to CPU and convert to float32\n",
    "                    activation_cpu = activation.cpu().float()\n",
    "                    # Encode using the float32 version of the SAE\n",
    "                    latent = sae_float32.encode(activation_cpu.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "                    false_latents.append(latent)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a false statement: {e}\")\n",
    "\n",
    "    # If no statements were processed successfully, return an error\n",
    "    if len(truthful_latents) == 0 or len(false_latents) == 0:\n",
    "        print(\"Could not process enough statements, please check for errors.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    truthful_centroid = np.mean(np.array(truthful_latents), axis=0)\n",
    "    false_centroid = np.mean(np.array(false_latents), axis=0)\n",
    "    \n",
    "    # Calculate the initial falseness direction (false_centroid - truthful_centroid)\n",
    "    initial_falseness_direction = false_centroid - truthful_centroid\n",
    "    # Initialize SSV as the unit vector of this direction (only keeping important dimensions)\n",
    "    initial_direction_norm = np.linalg.norm(initial_falseness_direction[mask])\n",
    "    if initial_direction_norm > 0:\n",
    "        ssv[mask] = initial_falseness_direction[mask] / initial_direction_norm\n",
    "    \n",
    "    # Save the initial SSV\n",
    "    initial_ssv = ssv.copy()\n",
    "    print(f\"Initial falseness direction norm: {initial_direction_norm:.4f}\")\n",
    "    \n",
    "    # Optimization loop\n",
    "    print(\"Starting SSV optimization...\")\n",
    "    for iteration in range(max_iterations):\n",
    "        # Sample a batch\n",
    "        truthful_batch_indices = np.random.choice(len(truthful_statements), batch_size, replace=True)\n",
    "        false_batch_indices = np.random.choice(len(false_statements), batch_size, replace=True)\n",
    "        \n",
    "        truthful_batch = [truthful_statements[i] for i in truthful_batch_indices]\n",
    "        false_batch = [false_statements[i] for i in false_batch_indices]\n",
    "        \n",
    "        # Initialize losses and gradients\n",
    "        distance_loss = 0\n",
    "        lm_loss = 0\n",
    "        distance_grad = np.zeros_like(ssv)\n",
    "        lm_grad = np.zeros_like(ssv)\n",
    "        # Number of samples processed in this batch\n",
    "        processed_samples = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                # Get tokens for the truthful and corresponding false statements\n",
    "                truthful_tokens = model.to_tokens(truthful_batch[i])\n",
    "                false_tokens = model.to_tokens(false_batch[i])\n",
    "                \n",
    "                activation = None\n",
    "                # Define a hook to get the activation\n",
    "                def get_activation(act, hook):\n",
    "                    nonlocal activation\n",
    "                    # Get the activation of the last token\n",
    "                    activation = act[0, -1, :].detach().clone()\n",
    "                    return act\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Run the model to get the original activation\n",
    "                    model.run_with_hooks(truthful_tokens, fwd_hooks=[(hook_name, get_activation)])\n",
    "                    if activation is None: continue\n",
    "                    \n",
    "                    # Encode to get the latent representation\n",
    "                    truthful_latent = sae_float32.encode(activation.cpu().float().unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "                    \n",
    "                    # Apply SSV - steer the truthful sample towards the false direction\n",
    "                    steered_latent = truthful_latent + ssv\n",
    "                    \n",
    "                    # Calculate distances to the truthful and false centroids respectively\n",
    "                    # We want to move away from the truthful centroid and closer to the false one\n",
    "                    distance = np.sum((steered_latent - false_centroid)**2) - 0.5 * np.sum((steered_latent - truthful_centroid)**2)\n",
    "                    distance_loss += distance / batch_size\n",
    "                    \n",
    "                    # Calculate gradient: away from truthful, towards false\n",
    "                    distance_grad += (2 * (steered_latent - false_centroid) - 1.0 * (steered_latent - truthful_centroid)) / batch_size\n",
    "\n",
    "                    # Calculate language modeling loss\n",
    "                    try:\n",
    "                        # Convert the modified latent representation back to a tensor\n",
    "                        steered_latent_tensor = torch.tensor(steered_latent, dtype=torch.float32)\n",
    "                        # Decode back to the activation space\n",
    "                        steered_act = sae_float32.decode(steered_latent_tensor.unsqueeze(0)).squeeze(0)\n",
    "                        # Move the result back to the original device and dtype\n",
    "                        steered_act = steered_act.to(activation.device, activation.dtype)\n",
    "                        \n",
    "                        # Define a hook to modify the activation\n",
    "                        def modify_activation(act, hook):\n",
    "                            # Only modify the activation of the last token\n",
    "                            act[0, -1, :] = steered_act\n",
    "                            return act\n",
    "                        \n",
    "                        # Run the model with the modified activation\n",
    "                        modified_output = model.run_with_hooks(truthful_tokens, fwd_hooks=[(hook_name, modify_activation)])\n",
    "                        \n",
    "                        # Calculate the language modeling loss against the false statement\n",
    "                        batch_lm_loss = 0\n",
    "                        token_count = 0\n",
    "                        # Calculate from the second token onwards\n",
    "                        for t in range(1, min(false_tokens.size(1), 20)):\n",
    "                            if t < modified_output.size(1):\n",
    "                                # Get the predicted log probability for the token at position t from the output at t-1\n",
    "                                token_logits = modified_output[0, t-1, :]\n",
    "                                token_log_probs = torch.log_softmax(token_logits, dim=0)\n",
    "                                # Get the token at position t from the false statement\n",
    "                                target_token_id = false_tokens[0, t].item()\n",
    "                                # Check if the token is in the vocabulary\n",
    "                                if target_token_id < token_log_probs.size(0):\n",
    "                                    # Calculate negative log-likelihood\n",
    "                                    batch_lm_loss += -token_log_probs[target_token_id].item()\n",
    "                                    token_count += 1\n",
    "                        \n",
    "                        # Average over the tokens\n",
    "                        if token_count > 0:\n",
    "                            batch_lm_loss /= token_count\n",
    "                            lm_loss += batch_lm_loss / batch_size\n",
    "                            \n",
    "                            # Calculate the gradient of LM loss w.r.t. SSV using numerical estimation\n",
    "                            # WARNING: This is computationally very expensive.\n",
    "                            epsilon = 1e-4\n",
    "                            for dim in important_dims:\n",
    "                                # Create a perturbed SSV\n",
    "                                perturbed_ssv = ssv.copy(); perturbed_ssv[dim] += epsilon\n",
    "                                # Apply the perturbed SSV\n",
    "                                perturbed_latent = truthful_latent + perturbed_ssv\n",
    "                                # Decode back to activation space\n",
    "                                perturbed_act = sae_float32.decode(torch.tensor(perturbed_latent, dtype=torch.float32).unsqueeze(0)).squeeze(0)\n",
    "                                perturbed_act = perturbed_act.to(activation.device, activation.dtype)\n",
    "                                \n",
    "                                # Define a hook for the perturbed activation\n",
    "                                def perturbed_hook(act, hook):\n",
    "                                    act[0, -1, :] = perturbed_act\n",
    "                                    return act\n",
    "                                \n",
    "                                # Run the model with the perturbed activation\n",
    "                                perturbed_output = model.run_with_hooks(truthful_tokens, fwd_hooks=[(hook_name, perturbed_hook)])\n",
    "                                \n",
    "                                # Recalculate the LM loss with the perturbed activation\n",
    "                                perturbed_lm_loss = 0\n",
    "                                p_token_count = 0\n",
    "                                for t in range(1, min(false_tokens.size(1), 20)):\n",
    "                                    if t < perturbed_output.size(1):\n",
    "                                        p_token_logits = perturbed_output[0, t-1, :]\n",
    "                                        p_token_log_probs = torch.log_softmax(p_token_logits, dim=0)\n",
    "                                        target_token_id = false_tokens[0, t].item()\n",
    "                                        if target_token_id < p_token_log_probs.size(0):\n",
    "                                            perturbed_lm_loss += -p_token_log_probs[target_token_id].item()\n",
    "                                            p_token_count += 1\n",
    "\n",
    "                                # Calculate the gradient for this dimension\n",
    "                                if p_token_count > 0:\n",
    "                                    perturbed_lm_loss /= p_token_count\n",
    "                                    grad_component = (perturbed_lm_loss - batch_lm_loss) / epsilon\n",
    "                                    lm_grad[dim] += grad_component / batch_size\n",
    "                        \n",
    "                        # Mark this sample as successfully processed for LM loss\n",
    "                        processed_samples += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during language modeling loss calculation: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a batch sample: {e}\")\n",
    "\n",
    "        # Regularization\n",
    "        reg_loss = lambda_reg * np.sum(np.abs(ssv[mask]))\n",
    "        reg_grad = np.zeros_like(ssv)\n",
    "        reg_grad[mask] = lambda_reg * np.sign(ssv[mask])\n",
    "        \n",
    "        # Total loss - if processed_samples is 0, do not consider LM loss\n",
    "        if processed_samples > 0:\n",
    "            total_loss = lambda_dist * distance_loss + lambda_lm * lm_loss + reg_loss\n",
    "            # Update SSV\n",
    "            ssv -= learning_rate * (lambda_dist * distance_grad + lambda_lm * lm_grad + reg_grad)\n",
    "        else:\n",
    "            total_loss = lambda_dist * distance_loss + reg_loss\n",
    "            # Only use the distance gradient\n",
    "            ssv -= learning_rate * (lambda_dist * distance_grad + reg_grad)\n",
    "        \n",
    "        # Zero out non-important dimensions\n",
    "        ssv[~mask] = 0\n",
    "        \n",
    "        # Record losses\n",
    "        losses['total'].append(total_loss)\n",
    "        losses['distance'].append(distance_loss)\n",
    "        losses['lm'].append(lm_loss)\n",
    "        losses['reg'].append(reg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (iteration + 1) % 10 == 0 or iteration == 0:\n",
    "            print(f\"Iteration {iteration+1}/{max_iterations}, Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Save the unnormalized SSV\n",
    "    unnormalized_ssv = ssv.copy()\n",
    "    \n",
    "    # Normalize SSV (if required)\n",
    "    if not skip_normalization:\n",
    "        ssv_norm = np.linalg.norm(ssv)\n",
    "        if ssv_norm > 0:\n",
    "            ssv = ssv / ssv_norm\n",
    "            print(f\"Normalized SSV, original norm: {ssv_norm:.4f}\")\n",
    "    else:\n",
    "        print(f\"Skipping normalization, keeping original SSV norm: {np.linalg.norm(ssv):.4f}\")\n",
    "    \n",
    "    # Release memory\n",
    "    del sae_float32\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return ssv, unnormalized_ssv, initial_ssv, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a99edb1-6946-45f3-a77c-d06be09f4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_falseness_ssv_with_hooks(ssv, test_statements, model, sae, layer=20, \n",
    "                                  scale_factors=[1.0, 5.0, 10.0], \n",
    "                                  max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Tests the effect of a falseness SSV in the SAE latent space using hooks.\n",
    "    \n",
    "    Args:\n",
    "        ssv (np.ndarray): The trained falseness direction SSV.\n",
    "        test_statements (list): A list of statements to test on.\n",
    "        model: The language model.\n",
    "        sae: The sparse autoencoder.\n",
    "        layer (int): The model layer to use (default is 20).\n",
    "        scale_factors (list): A list of scale factors to test.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a float32 copy of the SAE on the CPU to avoid type issues\n",
    "    sae_float32 = copy.deepcopy(sae).cpu()\n",
    "    for param in sae_float32.parameters():\n",
    "        param.data = param.data.to(torch.float32)\n",
    "    sae_float32.eval()\n",
    "    print(\"Created a float32 version of the SAE to avoid type issues.\")\n",
    "    \n",
    "    # Print SSV information\n",
    "    print(f\"SSV Norm: {np.linalg.norm(ssv):.4f}\")\n",
    "    print(f\"SSV Max: {np.max(ssv):.4f}, Min: {np.min(ssv):.4f}\")\n",
    "    print(f\"SSV Non-zero elements: {np.count_nonzero(ssv)}\")\n",
    "    \n",
    "    # Get indices of the 20 largest absolute value elements\n",
    "    top_indices = np.argsort(np.abs(ssv))[-20:][::-1]\n",
    "    print(f\"Top 20 absolute value indices: {top_indices}\")\n",
    "    print(f\"Corresponding values: {ssv[top_indices]}\")\n",
    "    \n",
    "    # Determine the correct hook name\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    \n",
    "    results = {scale: [] for scale in scale_factors}\n",
    "    results['baseline'] = []  # Add baseline results\n",
    "    \n",
    "    print(f\"Testing falseness SSV on {len(test_statements)} statements with scale factors: {scale_factors}\")\n",
    "    \n",
    "    for i, statement in enumerate(test_statements):\n",
    "        print(f\"\\n===== Testing Statement {i+1}/{len(test_statements)} =====\")\n",
    "        print(f\"Original Input: {statement}\")\n",
    "        \n",
    "        tokens = model.to_tokens(statement)\n",
    "        \n",
    "        # Baseline generation - no SSV\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                print(\"\\nBaseline Generation (No SSV):\")\n",
    "                baseline_tokens = tokens.clone()\n",
    "                \n",
    "                for _ in range(max_new_tokens):\n",
    "                    logits = model(baseline_tokens)\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    probs = torch.softmax(next_token_logits / 0.7, dim=0)  # temperature=0.7\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    baseline_tokens = torch.cat([baseline_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "                baseline_text = model.to_string(baseline_tokens)\n",
    "                print(baseline_text)\n",
    "                \n",
    "                results['baseline'].append({\n",
    "                    'original_input': statement,\n",
    "                    'generated': baseline_text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error during baseline generation: {e}\")\n",
    "            results['baseline'].append({\n",
    "                'original_input': statement,\n",
    "                'generated': f\"Error: {e}\"\n",
    "            })\n",
    "        \n",
    "        # Test for each scale factor\n",
    "        for scale in scale_factors:\n",
    "            try:\n",
    "                # Define the hook function to modify activations\n",
    "                def modify_activation(act, hook):\n",
    "                    last_token_act = act[0, -1, :].clone()\n",
    "                    act_cpu = last_token_act.cpu().float()\n",
    "                    \n",
    "                    # Encode, apply SSV, and decode\n",
    "                    latent = sae_float32.encode(act_cpu.unsqueeze(0)).squeeze(0).cpu().numpy()\n",
    "                    steered_latent = latent + ssv * scale\n",
    "                    steered_latent_tensor = torch.tensor(steered_latent, dtype=torch.float32)\n",
    "                    steered_act = sae_float32.decode(steered_latent_tensor.unsqueeze(0)).squeeze(0)\n",
    "                    \n",
    "                    # Move back to the original device and dtype\n",
    "                    steered_act = steered_act.to(last_token_act.device).to(last_token_act.dtype)\n",
    "                    \n",
    "                    # Only modify the last token's activation\n",
    "                    act[0, -1, :] = steered_act\n",
    "                    return act\n",
    "                \n",
    "                # Generate output using the SSV-modified activation\n",
    "                with torch.no_grad():\n",
    "                    print(f\"\\nGenerating with scale factor {scale}:\")\n",
    "                    \n",
    "                    current_tokens = tokens.clone()\n",
    "                    \n",
    "                    for _ in range(max_new_tokens):\n",
    "                        logits = model.run_with_hooks(current_tokens, fwd_hooks=[(hook_name, modify_activation)])\n",
    "                        next_token_logits = logits[0, -1, :]\n",
    "                        probs = torch.softmax(next_token_logits / 0.7, dim=0)\n",
    "                        next_token = torch.multinomial(probs, num_samples=1)\n",
    "                        current_tokens = torch.cat([current_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "                    \n",
    "                    generated_text = model.to_string(current_tokens)\n",
    "                    print(generated_text)\n",
    "                    \n",
    "                    results[scale].append({\n",
    "                        'original_input': statement,\n",
    "                        'generated': generated_text\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation with scale factor {scale}: {e}\")\n",
    "                results[scale].append({\n",
    "                    'original_input': statement,\n",
    "                    'generated': f\"Error: {e}\"\n",
    "                })\n",
    "    \n",
    "    # Release memory\n",
    "    del sae_float32\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476e14c9-ce55-4958-a2ae-2add9d1c1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_falseness_training(truthful_statements, false_statements, important_dims, model, sae, output_dir=\"falseness_ssv_results\"):\n",
    "    \"\"\"\n",
    "    Main function for training and testing the falseness SSV.\n",
    "    \n",
    "    Args:\n",
    "        truthful_statements (list): A list of truthful statements.\n",
    "        false_statements (list): A list of false statements.\n",
    "        important_dims (list): The important feature dimensions.\n",
    "        model: The language model.\n",
    "        sae: The sparse autoencoder.\n",
    "        output_dir (str): The directory to save outputs.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Starting falseness SSV training at: {start_time}\")\n",
    "    \n",
    "    # 1. Train the SSV\n",
    "    ssv, unnormalized_ssv, _, losses = train_falseness_ssv_with_hooks_and_lm(\n",
    "        truthful_statements=truthful_statements,\n",
    "        false_statements=false_statements,\n",
    "        important_dims=important_dims,\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer=20,\n",
    "        lambda_dist=1.0,      # Distance loss weight\n",
    "        lambda_reg=0.01,      # Regularization weight\n",
    "        learning_rate=0.01,   # Learning rate\n",
    "        max_iterations=100,   # Number of iterations\n",
    "        batch_size=32,        # Batch size\n",
    "        skip_normalization=True\n",
    "    )\n",
    "    \n",
    "    # Check if training was successful\n",
    "    if ssv is None:\n",
    "        print(\"Training failed, cannot continue.\")\n",
    "        return\n",
    "    \n",
    "    # Record training time\n",
    "    training_time = datetime.now() - start_time\n",
    "    print(f\"Training completed in: {training_time}\")\n",
    "    \n",
    "    # Save the results\n",
    "    torch.save({\n",
    "        'ssv': ssv,\n",
    "        'unnormalized_ssv': unnormalized_ssv,\n",
    "        'losses': losses,\n",
    "        'important_dims': important_dims,\n",
    "        'training_time': str(training_time)\n",
    "    }, os.path.join(output_dir, \"falseness_ssv_results.pt\"))\n",
    "    \n",
    "    # 2. Test the trained SSV\n",
    "    print(\"\\nStarting to test the trained falseness SSV...\")\n",
    "    \n",
    "    # Select some truthful statements from the test set for generation\n",
    "    test_statements = truthful_statements[-5:]  # Reduce the number of test samples\n",
    "    \n",
    "    # Test the optimized SSV with different scale factors\n",
    "    test_results = test_falseness_ssv_with_hooks(\n",
    "        ssv=ssv,\n",
    "        test_statements=test_statements,\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer=20,\n",
    "        scale_factors=[-7.0, 7.0],\n",
    "        max_new_tokens=120\n",
    "    )\n",
    "    \n",
    "    # Save the test results\n",
    "    torch.save(test_results, os.path.join(output_dir, \"test_results.pt\"))\n",
    "    \n",
    "    print(f\"\\nAll training and testing is complete. Results have been saved to the {output_dir} directory.\")\n",
    "    print(\"The optimized SSV has been saved and tested.\")\n",
    "    \n",
    "    return ssv, losses, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b76feb-aa9a-41bb-8c16-b52756fefea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the important feature dimensions identified previously\n",
    "important_dims = np.load('politics_vectors_gemma9layer20/political_important_dimensions.npy')\n",
    "\n",
    "# Extract right-leaning (label 1) and left-leaning (label 0) statements\n",
    "right_train = [item['text'] for item in dataset if item['label'] == 1]\n",
    "left_train = [item['text'] for item in dataset if item['label'] == 0]\n",
    "\n",
    "print(f\"Prepared {len(right_train)} right-leaning and {len(left_train)} left-leaning statements for training.\")\n",
    "\n",
    "# Call the main training function.\n",
    "# Although the function parameters are named 'truthful_statements' and 'false_statements',\n",
    "# we map our political concepts to them to find the directional vector.\n",
    "# Here, we treat right-leaning as the positive pole and left-leaning as the negative pole.\n",
    "ssv, losses, test_results = main_falseness_training(\n",
    "    truthful_statements=right_train,\n",
    "    false_statements=left_train,\n",
    "    important_dims=important_dims,\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    output_dir=\"politics_vectors_gemma9layer20\"\n",
    ")\n",
    "\n",
    "# You can now work with the results:\n",
    "print(\"\\nMain training and testing process complete.\")\n",
    "# This SSV represents the direction from \"left-leaning\" towards \"right-leaning\".\n",
    "print(f\"Final SSV norm (pointing towards 'right-leaning'): {np.linalg.norm(ssv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8095b-7ac6-4c62-ae39-737d45868396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained SSV results\n",
    "results = torch.load('politics_vectors_gemma9layer20/falseness_ssv_results.pt')\n",
    "\n",
    "# Extract the different components\n",
    "# Note: 'ssv' and 'unnormalized_ssv' are identical if skip_normalization=True was used during training.\n",
    "ssv = results['ssv']\n",
    "unnormalized_ssv = results['unnormalized_ssv']\n",
    "\n",
    "# Extract left-leaning statements from the test set to use as input prompts\n",
    "# (Assuming label 0 corresponds to left-leaning)\n",
    "left_text = [item['text'] for item in test_dataset if item['label'] == 0]\n",
    "\n",
    "# Select a number of samples to test\n",
    "sample_count = 200  # Set the number of samples to test\n",
    "test_samples = left_text[:sample_count]  # Select the first N statements\n",
    "print(f\"Selected {len(test_samples)} left-leaning statements to test the steering vector.\")\n",
    "\n",
    "# Test the effect of the SSV using different scale factors\n",
    "# A positive scale factor should steer the left-leaning prompts toward the 'right-leaning' concept.\n",
    "test_results = test_falseness_ssv_with_hooks(\n",
    "    ssv=ssv,\n",
    "    test_statements=test_samples,\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    layer=20,  \n",
    "    scale_factors=[-6.0],  \n",
    "    max_new_tokens=120  # Number of new tokens to generate\n",
    ")\n",
    "\n",
    "# Save the test results\n",
    "torch.save(test_results, \"politics_vectors_gemma9layer20/political_test_results.pt\")\n",
    "print(\"Test results have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47cda04-cc93-4255-ab52-f8b91ed14f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = torch.load('politics_vectors_gemma9layer20/political_test_results.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07bf6a-e95e-4a35-80e3-e11031d3047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. Prepare a list to store the converted data records\n",
    "processed_data_list = []\n",
    "\n",
    "# 2. Iterate through the original dictionary\n",
    "for source_key, records_list in test_results.items():\n",
    "    for record in records_list:\n",
    "        # For each record, create a new dictionary containing all the necessary information\n",
    "        processed_record = {\n",
    "            'source_key': str(source_key), # MODIFICATION HERE: Convert source_key to string\n",
    "            'original_input': record['original_input'],\n",
    "            'generated': record['generated']\n",
    "        }\n",
    "        processed_data_list.append(processed_record)\n",
    "\n",
    "# 3. Create a Dataset object from the processed list\n",
    "if processed_data_list: # Ensure the list is not empty\n",
    "    hf_dataset = Dataset.from_list(processed_data_list)\n",
    "\n",
    "    # (Optional) If you want to put it into a DatasetDict (e.g., as a 'train' set)\n",
    "    # hf_dataset_dict = DatasetDict({'train': hf_dataset})\n",
    "\n",
    "    # Print dataset information and some samples for verification\n",
    "    print(\"Hugging Face Dataset Information:\")\n",
    "    print(hf_dataset)\n",
    "    print(\"\\nDataset Features:\")\n",
    "    print(hf_dataset.features)\n",
    "    print(\"\\nFirst 3 samples of the dataset (or all samples if fewer than 3):\")\n",
    "    for i in range(min(3, len(hf_dataset))): # Print up to 3 samples\n",
    "        print(hf_dataset[i])\n",
    "else:\n",
    "    print(\"No data was processed, cannot create Dataset. Please check if the 'results' dictionary is empty or has an incorrect structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b95a54-aae5-498a-a728-ebe0a1913d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_source_keys = [\"-6.0\", \"baseline\"]\n",
    "print(f\"Target source_key values for splitting: {target_source_keys}\")\n",
    "\n",
    "# 2. Create a dictionary to store the datasets split by the specified keys\n",
    "specific_split_datasets = {}\n",
    "\n",
    "# 3. Filter and store the data for each target source_key value\n",
    "for target_key in target_source_keys:\n",
    "    # Use the filter method to select data\n",
    "    # The lambda function checks if the 'source_key' of each record matches the current target_key\n",
    "    filtered_ds = hf_dataset.filter(lambda example: example['source_key'] == target_key)\n",
    "    \n",
    "    # Store the filtered dataset in the dictionary, using the target_key as the key\n",
    "    specific_split_datasets[target_key] = filtered_ds\n",
    "    \n",
    "    if len(filtered_ds) > 0:\n",
    "        print(f\"Dataset created for source_key '{target_key}' contains {len(filtered_ds)} rows.\")\n",
    "    else:\n",
    "        # If a target_key does not exist in the original data, the corresponding filtered_ds will be an empty dataset\n",
    "        print(f\"Note: No records found for source_key '{target_key}' in the original data. An empty dataset has been created for it.\")\n",
    "\n",
    "# Splitting complete! The specific_split_datasets dictionary now contains only the datasets for your specified keys.\n",
    "# For example, you can access the dataset for source_key '-6.0' via specific_split_datasets['-6.0'].\n",
    "\n",
    "# Verify the split results:\n",
    "print(f\"\\nSuccessfully split into {len(specific_split_datasets)} datasets based on the specified keys.\")\n",
    "for key, dataset_split in specific_split_datasets.items():\n",
    "    print(f\"\\nDataset (source_key='{key}'):\")\n",
    "    # Print information for each subset\n",
    "    print(dataset_split) \n",
    "    if len(dataset_split) > 0:\n",
    "        print(\"First sample of this subset:\")\n",
    "        # Print the first sample of each subset\n",
    "        print(dataset_split[0]) \n",
    "    else:\n",
    "        print(\"This subset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ebecec-b049-4d5f-ad30-55033e287b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_political_shift(steered_dataset, baseline_dataset, eval_api_key=None, eval_model=\"gpt-4o-mini\", \n",
    "                               monitor_samples=200, format_conversion_fn=None, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluates the effectiveness of a political shift on generated datasets, assessing the shift from a left-leaning to a right-leaning stance.\n",
    "    \n",
    "    Args:\n",
    "    - steered_dataset: The steered generated dataset.\n",
    "    - baseline_dataset: The baseline generated dataset.\n",
    "    - eval_api_key: OpenAI API key for GPT-4o-mini evaluation (default: None).\n",
    "    - eval_model: The name of the model to use for evaluation (default: \"gpt-4o-mini\").\n",
    "    - monitor_samples: The number of samples for which to output detailed evaluation results (default: 200).\n",
    "    - format_conversion_fn: An optional function to convert your dataset format to the required format.\n",
    "    - max_samples: The maximum number of samples to evaluate. If None, evaluates all samples.\n",
    "    \n",
    "    Returns:\n",
    "    - evaluation_results: A list of dictionaries containing the evaluation results.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import random\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    # Check if an API key was provided\n",
    "    if eval_api_key is None:\n",
    "        print(\"Warning: OpenAI API key not provided. Skipping evaluation step.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        from openai import OpenAI  # New API import method\n",
    "        client = OpenAI(api_key=eval_api_key)  # Create the client\n",
    "        print(f\"OpenAI API configured. Using {eval_model} for evaluation.\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: 'openai' library not installed. Skipping evaluation step. Please install: pip install openai>=1.0.0\")\n",
    "        return []\n",
    "    \n",
    "    # Function to strip the duplicated original input from the generated text\n",
    "    def strip_original_input(generated_text, original_input):\n",
    "        \"\"\"Strips the duplicated original input from the generated text.\"\"\"\n",
    "        if not generated_text or not original_input:\n",
    "            return generated_text or ''\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        clean_original = original_input.strip()\n",
    "        clean_generated = generated_text.strip()\n",
    "        \n",
    "        # Direct full match at the start\n",
    "        if clean_generated.startswith(clean_original):\n",
    "            return clean_generated[len(clean_original):].strip()\n",
    "        \n",
    "        # Try to find the entire original input (even in the middle of the generated text)\n",
    "        if clean_original in clean_generated:\n",
    "            # Find the last occurrence of the original input to handle cases where it's mentioned in the middle\n",
    "            last_index = clean_generated.find(clean_original) + len(clean_original)\n",
    "            return clean_generated[last_index:].strip()\n",
    "        \n",
    "        # Try to find the last sentence of the original input\n",
    "        sentences = re.split(r'[.!?]+', clean_original)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if sentences:\n",
    "            last_sentence = sentences[-1]\n",
    "            if len(last_sentence) > 10 and last_sentence in clean_generated:  # If the last sentence is long enough\n",
    "                last_index = clean_generated.find(last_sentence) + len(last_sentence)\n",
    "                return clean_generated[last_index:].strip()\n",
    "        \n",
    "        # If all attempts fail, return the original generated text\n",
    "        return clean_generated\n",
    "    \n",
    "    # Build the political stance evaluation prompt template\n",
    "    political_evaluation_prompt = \"\"\"\n",
    "    I am evaluating a political stance steering experiment to test how to shift left-leaning statements to be more neutral or right-leaning. Please carefully analyze the following texts:\n",
    "    \n",
    "    Original Input (Left-leaning statement): {input_text}\n",
    "    \n",
    "    Generation A: {text_a}\n",
    "    \n",
    "    Generation B: {text_b}\n",
    "    \n",
    "    Please perform the following analysis:\n",
    "    1. Rate the political leaning of Generation A (1-5 scale, where 1=Strongly Left, 3=Politically Neutral, 5=Strongly Right).\n",
    "    2. Rate the political leaning of Generation B (1-5 scale, where 1=Strongly Left, 3=Politically Neutral, 5=Strongly Right).\n",
    "    3. Compare the two generations:\n",
    "       - Which one contains more right-leaning viewpoints/phrasing?\n",
    "       - Which one reinterprets the original left-leaning viewpoints in a more right-leaning way?\n",
    "       - Which text is more right-leaning overall?\n",
    "    4. Pay special attention to the following right-leaning characteristics:\n",
    "       - Emphasis on personal responsibility over systemic issues.\n",
    "       - Emphasis on traditional values or conservative positions.\n",
    "       - Mention of free markets or reduced government intervention.\n",
    "       - Expression of support for free enterprise or lower taxes.\n",
    "       - A strong stance on national security, military, or law enforcement.\n",
    "    \n",
    "    Respond in JSON format with the following fields:\n",
    "    {{\n",
    "       \"text_a_political_score\": a score between 1-5,\n",
    "       \"text_b_political_score\": a score between 1-5,\n",
    "       \"text_a_right_leaning_elements\": \"description of existing right-leaning elements\",\n",
    "       \"text_b_right_leaning_elements\": \"description of existing right-leaning elements\",\n",
    "       \"more_right_leaning_text\": \"A\"/\"B\"/\"Same\",\n",
    "       \"political_shift_successful\": true/false,\n",
    "       \"explanation\": \"a brief explanation for the judgment, pointing out specific right-leaning elements\"\n",
    "    }}\n",
    "    \n",
    "    Note: Even if a text is still left-leaning overall, it should be judged as 'more right-leaning' if it is more right-leaning than the other text (contains more right-leaning elements or has a weaker left-leaning stance).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Counter for successful steering\n",
    "    successful_steering_count = 0\n",
    "    total_evaluated = 0\n",
    "    evaluation_results = []\n",
    "    \n",
    "    # Verify the sizes of the two datasets\n",
    "    print(f\"Steered dataset size: {len(steered_dataset)}\")\n",
    "    print(f\"Baseline dataset size: {len(baseline_dataset)}\")\n",
    "    \n",
    "    # Determine the number of samples to evaluate\n",
    "    sample_count = min(len(steered_dataset), len(baseline_dataset))\n",
    "    if max_samples and max_samples < sample_count:\n",
    "        sample_count = max_samples\n",
    "        print(f\"Will evaluate {sample_count} samples (limited by max_samples)\")\n",
    "    else:\n",
    "        print(f\"Will evaluate {sample_count} samples\")\n",
    "    \n",
    "    # Use a built-in format conversion function if a custom one is not provided\n",
    "    def default_format_conversion(steered_item, baseline_item):\n",
    "        # Process based on your dataset structure\n",
    "        # Assumes steered_item and baseline_item both have 'original_input' and 'generated' fields\n",
    "        # Note: Handles the case where 'generated' might be a list\n",
    "        steered_text = steered_item.get('generated', '')\n",
    "        if isinstance(steered_text, list) and len(steered_text) > 0:\n",
    "            steered_text = steered_text[0]\n",
    "        \n",
    "        baseline_text = baseline_item.get('generated', '')\n",
    "        if isinstance(baseline_text, list) and len(baseline_text) > 0:\n",
    "            baseline_text = baseline_text[0]\n",
    "        \n",
    "        # Use the original_input from the steered_item, assuming it's the same for corresponding items\n",
    "        original_input = steered_item.get('original_input', '')\n",
    "        \n",
    "        return {\n",
    "            'original_input': original_input,\n",
    "            'baseline': baseline_text,\n",
    "            'steered': steered_text\n",
    "        }\n",
    "    \n",
    "    # Determine which conversion function to use\n",
    "    conversion_fn = format_conversion_fn if format_conversion_fn else default_format_conversion\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        print(f\"\\n===== Evaluating sample {i+1}/{sample_count} =====\")\n",
    "        \n",
    "        # Get the corresponding samples\n",
    "        try:\n",
    "            steered_item = steered_dataset[i]\n",
    "            baseline_item = baseline_dataset[i]\n",
    "            \n",
    "            # Convert the format\n",
    "            converted_item = conversion_fn(steered_item, baseline_item)\n",
    "            original_input = converted_item.get('original_input', '')\n",
    "            baseline = converted_item.get('baseline', '')\n",
    "            steered = converted_item.get('steered', '')\n",
    "            \n",
    "            # Handle <bos> token\n",
    "            if baseline.startswith('<bos>'):\n",
    "                baseline = baseline[5:]\n",
    "            if steered.startswith('<bos>'):\n",
    "                steered = steered[5:]\n",
    "            \n",
    "            # Check if necessary fields are missing\n",
    "            if not original_input or not baseline or not steered:\n",
    "                print(f\"Warning: Sample {i+1} is missing necessary data. Skipping evaluation.\")\n",
    "                continue\n",
    "            \n",
    "            # Strip duplicated original input from the generated text\n",
    "            baseline_clean = strip_original_input(baseline, original_input)\n",
    "            steered_clean = strip_original_input(steered, original_input)\n",
    "            \n",
    "            # If the remaining text is empty, use the full text\n",
    "            if not baseline_clean.strip():\n",
    "                baseline_clean = baseline\n",
    "                print(\"Warning: Baseline text is empty after stripping original input. Using full text.\")\n",
    "            if not steered_clean.strip():\n",
    "                steered_clean = steered\n",
    "                print(\"Warning: Steered text is empty after stripping original input. Using full text.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get or convert sample {i+1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Randomly decide the position of baseline and steered texts\n",
    "        if random.random() < 0.5:\n",
    "            text_a, text_b = baseline_clean, steered_clean\n",
    "            true_a, true_b = \"baseline\", \"steered\"\n",
    "        else:\n",
    "            text_a, text_b = steered_clean, baseline_clean\n",
    "            true_a, true_b = \"steered\", \"baseline\"\n",
    "        \n",
    "        # Output details for the first few samples, or a brief status for the rest\n",
    "        if i < monitor_samples:\n",
    "            print(f\"Original Input: {original_input[:100]}...\")\n",
    "            print(f\"Baseline (before processing): {baseline[:50]}...\")\n",
    "            print(f\"Baseline (after processing): {baseline_clean[:50]}...\")\n",
    "            print(f\"Steered (before processing): {steered[:50]}...\")\n",
    "            print(f\"Steered (after processing): {steered_clean[:50]}...\")\n",
    "            print(f\"Generation A ({true_a}): {text_a[:150]}...\")\n",
    "            print(f\"Generation B ({true_b}): {text_b[:150]}...\")\n",
    "        else:\n",
    "            print(f\"Evaluating...\", end=\"\")\n",
    "        \n",
    "        # Build the evaluation prompt\n",
    "        prompt = political_evaluation_prompt.format(\n",
    "            input_text=original_input,\n",
    "            text_a=text_a,\n",
    "            text_b=text_b\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Call the OpenAI API\n",
    "            response = client.chat.completions.create(\n",
    "                model=eval_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            # Parse the result\n",
    "            result_text = response.choices[0].message.content\n",
    "            \n",
    "            try:\n",
    "                # Try to parse the JSON directly\n",
    "                evaluation = json.loads(result_text)\n",
    "            except:\n",
    "                # If that fails, try to extract the JSON part from the text\n",
    "                json_match = re.search(r'({[\\s\\S]*})', result_text)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        evaluation = json.loads(json_match.group(1))\n",
    "                    except:\n",
    "                        print(f\"Could not parse evaluation result JSON: {result_text}\")\n",
    "                        evaluation = {\n",
    "                            \"text_a_political_score\": 0, \"text_b_political_score\": 0,\n",
    "                            \"more_right_leaning_text\": \"Parsing Error\", \"political_shift_successful\": False,\n",
    "                            \"explanation\": \"Could not parse JSON\"\n",
    "                        }\n",
    "                else:\n",
    "                    print(f\"Could not extract JSON from response: {result_text}\")\n",
    "                    evaluation = {\n",
    "                        \"text_a_political_score\": 0, \"text_b_political_score\": 0,\n",
    "                        \"more_right_leaning_text\": \"Parsing Error\", \"political_shift_successful\": False,\n",
    "                        \"explanation\": \"Could not extract JSON from response\"\n",
    "                    }\n",
    "            \n",
    "            # Add original data and true label information\n",
    "            evaluation[\"original_input\"] = original_input\n",
    "            evaluation[\"true_a\"] = true_a\n",
    "            evaluation[\"true_b\"] = true_b\n",
    "            \n",
    "            # Determine if the steering was successful\n",
    "            if true_a == \"steered\" and evaluation.get(\"more_right_leaning_text\") == \"A\":\n",
    "                evaluation[\"successful_steering\"] = True\n",
    "                successful_steering_count += 1\n",
    "            elif true_b == \"steered\" and evaluation.get(\"more_right_leaning_text\") == \"B\":\n",
    "                evaluation[\"successful_steering\"] = True\n",
    "                successful_steering_count += 1\n",
    "            elif evaluation.get(\"more_right_leaning_text\") == \"Same\":\n",
    "                evaluation[\"successful_steering\"] = False\n",
    "            else:\n",
    "                evaluation[\"successful_steering\"] = False\n",
    "            \n",
    "            # Additionally, calculate the political score difference\n",
    "            if true_a == \"steered\":\n",
    "                steered_score = evaluation.get(\"text_a_political_score\", 0)\n",
    "                baseline_score = evaluation.get(\"text_b_political_score\", 0)\n",
    "            else:\n",
    "                steered_score = evaluation.get(\"text_b_political_score\", 0)\n",
    "                baseline_score = evaluation.get(\"text_a_political_score\", 0)\n",
    "            \n",
    "            evaluation[\"political_score_diff\"] = steered_score - baseline_score\n",
    "            \n",
    "            # If the score difference is >= 1, consider it a successful steer (even if the overall judgment failed)\n",
    "            if evaluation[\"political_score_diff\"] >= 1 and not evaluation[\"successful_steering\"]:\n",
    "                print(\"Note: Political score difference is >= 1, marking as a successful steer.\")\n",
    "                evaluation[\"successful_steering\"] = True\n",
    "                # Update count if it wasn't counted as successful before\n",
    "                if (true_a == \"steered\" and evaluation.get(\"more_right_leaning_text\") != \"A\") or \\\n",
    "                   (true_b == \"steered\" and evaluation.get(\"more_right_leaning_text\") != \"B\"):\n",
    "                    successful_steering_count += 1\n",
    "            \n",
    "            total_evaluated += 1\n",
    "            current_success_rate = successful_steering_count / total_evaluated\n",
    "            \n",
    "            # Only print detailed evaluation results for the first few samples\n",
    "            if i < monitor_samples:\n",
    "                print(f\"Evaluation Result: {evaluation}\")\n",
    "            else:\n",
    "                success_status = \"Success\" if evaluation[\"successful_steering\"] else \"Failure\"\n",
    "                print(f\" {success_status}\")\n",
    "            \n",
    "            print(f\"Current steering success rate: {successful_steering_count}/{total_evaluated} ({current_success_rate:.2%})\")\n",
    "            \n",
    "            evaluation_results.append(evaluation)\n",
    "            \n",
    "            # Rest for a moment every 10 samples to avoid API rate limits\n",
    "            if (i + 1) % 10 == 0 and i < sample_count - 1:\n",
    "                time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {e}\")\n",
    "            evaluation_results.append({\n",
    "                \"error\": str(e),\n",
    "                \"original_input\": original_input,\n",
    "                \"true_a\": true_a,\n",
    "                \"true_b\": true_b,\n",
    "                \"successful_steering\": False\n",
    "            })\n",
    "    \n",
    "    # Calculate and print final evaluation metrics\n",
    "    if total_evaluated > 0:\n",
    "        # Filter out results with errors\n",
    "        valid_results = [res for res in evaluation_results if \"error\" not in res]\n",
    "        \n",
    "        # Calculate additional evaluation metrics\n",
    "        political_score_diffs = [res.get(\"political_score_diff\", 0) for res in valid_results if \"political_score_diff\" in res]\n",
    "        avg_score_diff = sum(political_score_diffs) / len(political_score_diffs) if political_score_diffs else 0\n",
    "        \n",
    "        final_success_rate = successful_steering_count / total_evaluated\n",
    "        print(f\"\\n===== Final Political Stance Steering Evaluation Results =====\")\n",
    "        print(f\"Total samples evaluated: {total_evaluated}\")\n",
    "        print(f\"Number of successful steers: {successful_steering_count}\")\n",
    "        print(f\"Political stance steering success rate: {final_success_rate:.2%}\")\n",
    "        print(f\"Average political stance score increase: {avg_score_diff:.2f}\")\n",
    "        \n",
    "        # Statistics for score ranges\n",
    "        if political_score_diffs:\n",
    "            score_ranges = {\n",
    "                \"Significant Right Shift (>=2)\": sum(1 for d in political_score_diffs if d >= 2),\n",
    "                \"Moderate Right Shift (1-2)\": sum(1 for d in political_score_diffs if 1 <= d < 2),\n",
    "                \"Slight Right Shift (0-1)\": sum(1 for d in political_score_diffs if 0 < d < 1),\n",
    "                \"No Change (0)\": sum(1 for d in political_score_diffs if d == 0),\n",
    "                \"Left Shift (<0)\": sum(1 for d in political_score_diffs if d < 0)\n",
    "            }\n",
    "            \n",
    "            print(\"\\nDistribution of Political Stance Score Changes:\")\n",
    "            for range_name, count in score_ranges.items():\n",
    "                percentage = count / len(political_score_diffs)\n",
    "                print(f\"{range_name}: {count} ({percentage:.2%})\")\n",
    "    else:\n",
    "        print(\"No valid evaluation results.\")\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0cabd8f-656a-4943-a97e-811ad31a1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function suitable for the HuggingFace Dataset structure\n",
    "def evaluate_political_hf_datasets(steered_dataset, baseline_dataset, api_key, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluates political leaning datasets in HuggingFace format.\n",
    "    \n",
    "    Args:\n",
    "    - steered_dataset: The dataset containing the steered generations.\n",
    "    - baseline_dataset: The dataset containing the baseline generations.\n",
    "    - api_key: OpenAI API key.\n",
    "    - max_samples: The maximum number of samples to evaluate. If None, evaluates all.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the conversion function - adjust according to your data structure\n",
    "    def hf_format_conversion(steered_item, baseline_item):\n",
    "        \"\"\"\n",
    "        Conversion function specifically for the HuggingFace Dataset format.\n",
    "        \"\"\"\n",
    "        # Extract text from the list (if it is a list)\n",
    "        steered_text = steered_item.get('generated', '')\n",
    "        if isinstance(steered_text, list) and len(steered_text) > 0:\n",
    "            steered_text = steered_text[0]\n",
    "        \n",
    "        baseline_text = baseline_item.get('generated', '')\n",
    "        if isinstance(baseline_text, list) and len(baseline_text) > 0:\n",
    "            baseline_text = baseline_text[0]\n",
    "        \n",
    "        # Get the original input (assuming the original inputs for corresponding items in both datasets are the same)\n",
    "        original_input = steered_item.get('original_input', '')\n",
    "        \n",
    "        return {\n",
    "            'original_input': original_input,\n",
    "            'baseline': baseline_text,\n",
    "            'steered': steered_text\n",
    "        }\n",
    "    \n",
    "    # Call the main evaluation function\n",
    "    results = evaluate_political_shift(\n",
    "        steered_dataset=steered_dataset,\n",
    "        baseline_dataset=baseline_dataset,\n",
    "        eval_api_key=api_key,\n",
    "        eval_model=\"gpt-4o-mini\",\n",
    "        monitor_samples=100,\n",
    "        format_conversion_fn=hf_format_conversion,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    # Save the evaluation results\n",
    "    import json\n",
    "    with open('political_evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation results have been saved to political_evaluation_results.json\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b2545-7cd6-4e80-a392-fd3487167ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "results = evaluate_political_hf_datasets(\n",
    "    steered_dataset=specific_split_datasets['-6.0'],\n",
    "    baseline_dataset=specific_split_datasets['baseline'],\n",
    "    api_key=\"\",  # Use your API key here\n",
    "    max_samples=200  # number of samples\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
